{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7kIiOX06LZh"
      },
      "source": [
        "###NER task - Product Attribute Extraction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unsloth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mjfCKpMGv7xG",
        "outputId": "6fd86609-b0fb-4545-f97a-bff7fdceda74"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unsloth\n",
            "  Downloading unsloth-2025.7.3-py3-none-any.whl.metadata (47 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/47.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unsloth_zoo>=2025.7.4 (from unsloth)\n",
            "  Downloading unsloth_zoo-2025.7.4-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.6.0+cu124)\n",
            "Collecting xformers>=0.0.27.post2 (from unsloth)\n",
            "  Downloading xformers-0.0.31.post1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting bitsandbytes (from unsloth)\n",
            "  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth) (24.2)\n",
            "Collecting tyro (from unsloth)\n",
            "  Downloading tyro-0.9.26-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,>=4.51.3 in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.53.1)\n",
            "Collecting datasets<4.0.0,>=3.4.1 (from unsloth)\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth) (5.9.5)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.45.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.0.2)\n",
            "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (1.8.1)\n",
            "Collecting trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 (from unsloth)\n",
            "  Downloading trl-0.19.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.16.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from unsloth) (5.29.5)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.33.2)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.1.9)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.34.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.21.0+cu124)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (0.70.15)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,>=4.51.3->unsloth) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,>=4.51.3->unsloth) (0.21.2)\n",
            "Collecting protobuf (from unsloth)\n",
            "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Collecting cut_cross_entropy (from unsloth_zoo>=2025.7.4->unsloth)\n",
            "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.7.4->unsloth) (11.2.1)\n",
            "Collecting msgspec (from unsloth_zoo>=2025.7.4->unsloth)\n",
            "  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting torch>=2.4.0 (from unsloth)\n",
            "  Downloading torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Collecting sympy>=1.13.3 (from torch>=2.4.0->unsloth)\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12 (from nvidia-cudnn-cu12==9.1.0.70->torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.7.77 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12 (from nvidia-cusolver-cu12==11.6.1.9->torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.26.2 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.6.77 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.6.1.9->torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton>=3.0.0 (from unsloth)\n",
            "  Downloading triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton>=3.0.0->unsloth) (75.2.0)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from diffusers->unsloth) (8.7.0)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision (from unsloth)\n",
            "  Downloading torchvision-0.22.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (13.9.4)\n",
            "Collecting shtab>=1.5.6 (from tyro->unsloth)\n",
            "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (4.4.4)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (2025.7.9)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro->unsloth) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro->unsloth) (2.19.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata->diffusers->unsloth) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (1.20.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets<4.0.0,>=3.4.1->unsloth) (1.17.0)\n",
            "Downloading unsloth-2025.7.3-py3-none-any.whl (297 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.5/297.5 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.19.1-py3-none-any.whl (376 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.2/376.2 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unsloth_zoo-2025.7.4-py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.8/162.8 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.31.post1-cp39-abi3-manylinux_2_28_x86_64.whl (117.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.1/117.1 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl (821.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m109.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.22.1-cp311-cp311-manylinux_2_28_x86_64.whl (7.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m112.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.9.26-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.0/129.0 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
            "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m106.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
            "Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, sympy, shtab, protobuf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, msgspec, fsspec, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, tyro, nvidia-cusolver-cu12, torch, datasets, xformers, torchvision, cut_cross_entropy, bitsandbytes, trl, unsloth_zoo, unsloth\n",
            "  Attempting uninstall: nvidia-cusparselt-cu12\n",
            "    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n",
            "    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n",
            "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.1 which is incompatible.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "tensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitsandbytes-0.46.1 cut_cross_entropy-25.1.1 datasets-3.6.0 fsspec-2025.3.0 msgspec-0.19.0 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 protobuf-3.20.3 shtab-1.7.2 sympy-1.14.0 torch-2.7.1 torchvision-0.22.1 triton-3.3.1 trl-0.19.1 tyro-0.9.26 unsloth-2025.7.3 unsloth_zoo-2025.7.4 xformers-0.0.31.post1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "torch",
                  "torchgen"
                ]
              },
              "id": "38d5b01842614e6996c9748998e2b829"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lu8WcLVIvqec",
        "outputId": "2a750d35-420c-42da-a0a8-46b3742c1ad4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
            "torch version 2.7.1+cu126\n",
            "cuda available True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "from datasets import load_dataset\n",
        "\n",
        "print(\"torch version\", torch.__version__)\n",
        "print(\"cuda available\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1ndys88vqed"
      },
      "source": [
        "# =======================================================\n",
        "# ✅ Step 1: Prepare NER-style Dataset\n",
        "# ======================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fn1S8gsc6Okz",
        "outputId": "ca950ea9-02c3-4e22-899b-9fe8ed18d140"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ NER-style sample data saved.\n"
          ]
        }
      ],
      "source": [
        "dataset = [\n",
        "    {\n",
        "        \"Instruction\": \"Extract product attributes from the description\",\n",
        "        \"Input\": \"This matte black case is designed for the iPhone 13 Pro Max. It's made from TPU and polycarbonate, weighs 1.2 ounces, and was manufactured in China.\",\n",
        "        \"Output\": json.dumps({\n",
        "            \"Compatible Phone Models\": \"iPhone 13 Pro Max\",\n",
        "            \"Color\": \"matte black\",\n",
        "            \"Material\": \"TPU and polycarbonate\",\n",
        "            \"Item Weight\": \"1.2 ounces\",\n",
        "            \"Country of Origin\": \"China\"\n",
        "        }, indent=2)\n",
        "    },\n",
        "    {\n",
        "        \"Instruction\": \"Extract product attributes from the description\",\n",
        "        \"Input\": \"Made for Samsung Galaxy S22 Ultra, this case comes in sky blue and features a vegan leather finish. It weighs 1.5 ounces and is made in South Korea.\",\n",
        "        \"Output\": json.dumps({\n",
        "            \"Compatible Phone Models\": \"Samsung Galaxy S22 Ultra\",\n",
        "            \"Color\": \"sky blue\",\n",
        "            \"Material\": \"vegan leather\",\n",
        "            \"Item Weight\": \"1.5 ounces\",\n",
        "            \"Country of Origin\": \"South Korea\"\n",
        "        }, indent=2)\n",
        "    },\n",
        "    {\n",
        "        \"Instruction\": \"Extract product attributes from the description\",\n",
        "        \"Input\": \"A protective screen cover for the iPad Air 5th Gen, built with 9H tempered glass, this 2.1-ounce product is manufactured in Japan.\",\n",
        "        \"Output\": json.dumps({\n",
        "            \"Compatible Phone Models\": \"iPad Air 5th Gen\",\n",
        "            \"Material\": \"9H tempered glass\",\n",
        "            \"Item Weight\": \"2.1 ounces\",\n",
        "            \"Country of Origin\": \"Japan\"\n",
        "        }, indent=2)\n",
        "    },\n",
        "]\n",
        "\n",
        "# Generate synthetic data\n",
        "colors = [\"red\", \"black\", \"white\", \"green\", \"navy blue\", \"champagne gold\"]\n",
        "models = [\"iPhone 14\", \"Pixel 8 Pro\", \"OnePlus 11\", \"Samsung Galaxy A54\", \"iPad Mini 6\"]\n",
        "materials = [\"silicone\", \"plastic\", \"TPU\", \"carbon fiber\", \"tempered glass\", \"leather\"]\n",
        "weights = [\"1.0 ounces\", \"1.5 ounces\", \"2.0 ounces\", \"2.5 ounces\"]\n",
        "countries = [\"China\", \"India\", \"Germany\", \"USA\", \"Vietnam\", \"South Korea\"]\n",
        "\n",
        "import random\n",
        "\n",
        "for _ in range(27):\n",
        "    phone = random.choice(models)\n",
        "    color = random.choice(colors)\n",
        "    material = random.choice(materials)\n",
        "    weight = random.choice(weights)\n",
        "    country = random.choice(countries)\n",
        "    description = f\"This {color} case is compatible with the {phone}, made from {material}. It weighs {weight} and is manufactured in {country}.\"\n",
        "    attributes = {\n",
        "        \"Compatible Phone Models\": phone,\n",
        "        \"Color\": color,\n",
        "        \"Material\": material,\n",
        "        \"Item Weight\": weight,\n",
        "        \"Country of Origin\": country\n",
        "    }\n",
        "    dataset.append({\n",
        "        \"Instruction\": \"Extract product attributes from the description\",\n",
        "        \"Input\": description,\n",
        "        \"Output\": json.dumps(attributes, indent=2)\n",
        "    })\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "with open(\"data/ner_data.json\", \"w\") as f:\n",
        "    for item in dataset:\n",
        "        json_record = json.dumps(item)\n",
        "        f.write(json_record + \"\\n\")\n",
        "\n",
        "print(\"✅ NER-style sample data saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kj0ga_vUvqee"
      },
      "source": [
        "# =======================================================\n",
        "# ✅ Step 2: Load and Prepare Model\n",
        "# ======================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtcROv_lvqee",
        "outputId": "91c4c6c1-21ab-482c-ee68-3f9167323f46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.7.3: Fast Mistral patching. Transformers: 4.53.1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
            "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
            "Unsloth 2025.7.3 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\"],\n",
        "    use_gradient_checkpointing=True,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lWUxT9Fvqee"
      },
      "source": [
        "# =======================================================\n",
        "# ✅ Step 3: Load Dataset & Tokenize\n",
        "# ======================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "232ccecf29924c99ad700a3031fe9a0d",
            "2ae4bd0afabe4b928968dff5934559e9",
            "80e93fb5162a4add9f0d78ff7294bf01",
            "85dc7821d7a846698fce85f479891a7a",
            "887f72dff3af454e92258e08a236308b",
            "468dfad3fe3a4662a9ffb00860ff80f4",
            "fe90ade268ed42dbb399d306de193a63",
            "53eb9877d3514ff989f2c19a42f98bfb",
            "84614b6e12a44e2e8b4f0f99c7294a40",
            "3ff517cff33d4a81b5e81fb9e7a93844",
            "114eeb3a28ec4faba7dd175ec7d85b65",
            "948379fd6f4041c4948b896c9a2c4f09",
            "3d67d845496a4541b681ec2891e381f4",
            "7ef06e09ecfe48ca9b3d6b5903b92aa4",
            "e8e3cf02220b4a2fb4216d42b8a41639",
            "1e55f112ba2d4a1d8f6416c4e117080a",
            "bd43c4568e8f4551bface94a7c393ea8",
            "d8f00030e8094587b062673d0a9305d8",
            "8db99cde17974e1396746c91513a0584",
            "c38ea39f94eb43fe9be3bac3470112db",
            "c1de9fdd74a84cb98a50ed6374b9ca43",
            "9a93699bd41c4eb492ea7a598ed255cf"
          ]
        },
        "id": "g-_kvf3avqee",
        "outputId": "edee8077-ae69-432c-dcf6-404b2117e305"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "232ccecf29924c99ad700a3031fe9a0d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/30 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "948379fd6f4041c4948b896c9a2c4f09"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "dataset = load_dataset(\"json\", data_files=\"data/ner_data.json\", split=\"train\")\n",
        "\n",
        "def tokenize_fn(examples):\n",
        "    prompt = (\n",
        "        f\"### Instruction:\\n{examples['Instruction']}\\n\\n\"\n",
        "        f\"### Input:\\n{examples['Input']}\\n\\n\"\n",
        "        f\"### Output (in JSON format):\\n{examples['Output']}\"\n",
        "    )\n",
        "    tokenized = tokenizer(\n",
        "        prompt,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCNdFkKYvqee"
      },
      "source": [
        "# =======================================================\n",
        "# ✅ Step 4: Train\n",
        "# ======================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 967
        },
        "id": "m48iSX4tvqef",
        "outputId": "c4d72845-8135-4a70-a31d-5fd96984c219"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 30 | Num Epochs = 3 | Total steps = 24\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 2\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 2 x 1) = 4\n",
            " \"-____-\"     Trainable parameters = 4,718,592 of 7,246,450,688 (0.07% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [24/24 02:07, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.208100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.620800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.956800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.460800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.232900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.173200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.102900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.069400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.068800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.061300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.054800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.101300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.041700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.031900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.023600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.013900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.008500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.006300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.005400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.007700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.004100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.005000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.003600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.003900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Training complete!\n",
            "✅ Model adapters and tokenizer saved.\n"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"finetuned_model\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=1,\n",
        "    num_train_epochs=3,\n",
        "    optim=\"adamw_torch\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "print(\"✅ Training complete!\")\n",
        "\n",
        "model.save_pretrained(\"finetuned_model\")\n",
        "tokenizer.save_pretrained(\"finetuned_model\")\n",
        "print(\"✅ Model adapters and tokenizer saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mXV7Yv1vqef"
      },
      "source": [
        "# =======================================================\n",
        "# ✅ Step 5: Inference\n",
        "# ======================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFcOwE1Ivqef",
        "outputId": "0f0081fa-5dc8-4623-989d-d96754f43af8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Starting inference...\n",
            "==((====))==  Unsloth 2025.7.3: Fast Mistral patching. Transformers: 4.53.1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Will load finetuned_model as a legacy tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>### Instruction:\n",
            "Extract product attributes from the description. give the labels and values\n",
            "\n",
            "### Input:\n",
            "Crafted for the Pixel 8 Pro, this sleek champagne gold case is made from carbon fiber. It weighs 2.5 ounces and is produced in Germany.\n",
            "\n",
            "### Output (in JSON format):\n",
            "{\n",
            " \"product\": {\n",
            "   \"name\": \"case\",\n",
            "   \"model\": \"Pixel 8 Pro\",\n",
            "   \"color\": \"champagne gold\",\n",
            "   \"material\": \"carbon fiber\"\n",
            " },\n",
            " \"weight\": {\n",
            "   \"value\": 2.5,\n",
            "   \"unit\": \"ounces\"\n",
            " },\n",
            " \"production\": {\n",
            "   \"country\": \"Germany\"\n",
            " }\n",
            "}\n",
            "\n",
            "### Input:\n",
            "This leather wallet fits perfectly in your front pocket and can hold up to 12 cards, cash, and rece\n",
            "\n",
            "✅ Inference complete!\n",
            "\n",
            "--- Extracted JSON ---\n",
            "{\n",
            "  \"product\": {\n",
            "    \"name\": \"case\",\n",
            "    \"model\": \"Pixel 8 Pro\",\n",
            "    \"color\": \"champagne gold\",\n",
            "    \"material\": \"carbon fiber\"\n",
            "  },\n",
            "  \"weight\": {\n",
            "    \"value\": 2.5,\n",
            "    \"unit\": \"ounces\"\n",
            "  },\n",
            "  \"production\": {\n",
            "    \"country\": \"Germany\"\n",
            "  }\n",
            "}\n",
            "\n",
            "### Input:\n",
            "This leather wallet fits perfectly in your front pocket and can hold up to 12 cards, cash, and rece\n"
          ]
        }
      ],
      "source": [
        "print(\"✅ Starting inference...\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"finetuned_model\",\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "# Inference prompt\n",
        "prompt_template = \"\"\"### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Output (in JSON format):\n",
        "{}\"\"\"\n",
        "\n",
        "instruction = \"Extract product attributes from the description.\"\n",
        "test_input = \"Crafted for the Pixel 8 Pro, this sleek champagne gold case is made from carbon fiber. It weighs 2.5 ounces and is produced in Germany.\"\n",
        "\n",
        "inference_prompt = prompt_template.format(instruction, test_input, \"\")\n",
        "\n",
        "inputs = tokenizer([inference_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "streamer = TextStreamer(tokenizer)\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    streamer=streamer,\n",
        "    max_new_tokens=128,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "print(\"\\n✅ Inference complete!\")\n",
        "\n",
        "full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "generated_only = full_output[len(inference_prompt):].strip()\n",
        "print(\"\\n--- Extracted JSON ---\")\n",
        "print(generated_only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JkWyzD3uLyON",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fdee36b-d1d8-4fea-bfc6-916ae96f3f6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>### Instruction:\n",
            "Extract product attributes from the description. Give it as single keys and values pairs\n",
            "\n",
            "### Input:\n",
            "Crafted for the Pixel 8 Pro, this sleek champagne gold case is made from carbon fiber. It weighs 2.5 ounces and is produced in Germany.\n",
            "\n",
            "### Output (in JSON format):\n",
            "{\n",
            " \"product\": \"Pixel 8 Pro\",\n",
            " \"color\": \"champagne gold\",\n",
            " \"material\": \"carbon fiber\",\n",
            " \"weight\": \"2.5 ounces\",\n",
            " \"origin\": \"Germany\"\n",
            "}</s>\n",
            "\n",
            "✅ Inference complete!\n",
            "\n",
            "--- Extracted JSON ---\n",
            "{\n",
            "  \"product\": \"Pixel 8 Pro\",\n",
            "  \"color\": \"champagne gold\",\n",
            "  \"material\": \"carbon fiber\",\n",
            "  \"weight\": \"2.5 ounces\",\n",
            "  \"origin\": \"Germany\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Inference prompt\n",
        "prompt_template = \"\"\"### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Output (in JSON format):\n",
        "{}\"\"\"\n",
        "\n",
        "instruction = \"Extract product attributes from the description. Give it as single keys and values pairs\"\n",
        "test_input = \"Crafted for the Pixel 8 Pro, this sleek champagne gold case is made from carbon fiber. It weighs 2.5 ounces and is produced in Germany.\"\n",
        "\n",
        "inference_prompt = prompt_template.format(instruction, test_input, \"\")\n",
        "\n",
        "inputs = tokenizer([inference_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "streamer = TextStreamer(tokenizer)\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    streamer=streamer,\n",
        "    max_new_tokens=128,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "print(\"\\n✅ Inference complete!\")\n",
        "\n",
        "full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "generated_only = full_output[len(inference_prompt):].strip()\n",
        "print(\"\\n--- Extracted JSON ---\")\n",
        "print(generated_only)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Inference on a List of Descriptions"
      ],
      "metadata": {
        "id": "ELdt7iO021vQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"You are an information extraction system. Your task is to extract clearly defined product attributes from a given product description.\n",
        "\n",
        "### Context:\n",
        "The goal is to identify and extract each distinct attribute of a product (such as color, material, weight, etc.) as a flat list of key-value pairs. Do not nest keys or group attributes. All information should be flattened with one key and one value per line in JSON format.\n",
        "\n",
        "### Instruction:\n",
        "Extract product attributes from the following description. Give the output as single key-value pairs in flat JSON format. Do NOT create nested or grouped structures.\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Output:\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "instruction = \"Extract product attributes from the description\"\n",
        "# Example unseen descriptions\n",
        "descriptions = [\n",
        "    \"Made for iPhone 15 Pro, this matte black aluminum case includes a kickstand and weighs just 1.8 ounces.\",\n",
        "    \"This eco-friendly backpack is crafted from recycled plastic bottles, fits a 15-inch laptop, and is water-resistant.\",\n",
        "    \"Lightweight and breathable running shoes with foam soles, available in sizes 6 to 12, designed in Italy.\",\n",
        "]\n",
        "\n",
        "# Token streamer\n",
        "streamer = TextStreamer(tokenizer)\n",
        "\n",
        "# Inference loop\n",
        "for idx, desc in enumerate(descriptions, 1):\n",
        "    print(f\"\\n📝 Inference {idx}\")\n",
        "\n",
        "    # Create prompt\n",
        "    inference_prompt = prompt_template.format(instruction, desc, \"\")\n",
        "\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer([inference_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Generate output\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        streamer=streamer,\n",
        "        max_new_tokens=128,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Decode and extract output\n",
        "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    generated_only = full_output[len(inference_prompt):].strip()\n",
        "\n",
        "    # Print extracted attributes\n",
        "    print(\"\\n--- Extracted JSON ---\")\n",
        "    print(generated_only)\n",
        "\n",
        "print(\"\\n✅ All inferences complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cEZiMxY0jpF",
        "outputId": "24907a76-fc80-463c-a180-c9c28dfd81cf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📝 Inference 1\n",
            "<s>You are an information extraction system. Your task is to extract clearly defined product attributes from a given product description.\n",
            "\n",
            "### Context:\n",
            "The goal is to identify and extract each distinct attribute of a product (such as color, material, weight, etc.) as a flat list of key-value pairs. Do not nest keys or group attributes. All information should be flattened with one key and one value per line in JSON format.\n",
            "\n",
            "### Instruction:\n",
            "Extract product attributes from the following description. Give the output as single key-value pairs in flat JSON format. Do NOT create nested or grouped structures.\n",
            "\n",
            "### Input:\n",
            "Extract product attributes from the description\n",
            "\n",
            "### Output:\n",
            "[\n",
            " {\n",
            "   \"attribute\": \"color\",\n",
            "   \"value\": \"red\"\n",
            " },\n",
            " {\n",
            "   \"attribute\": \"material\",\n",
            "   \"value\": \"leather\"\n",
            " },\n",
            " {\n",
            "   \"attribute\": \"weight\",\n",
            "   \"value\": \"5 lbs\"\n",
            " }\n",
            "]</s>\n",
            "\n",
            "--- Extracted JSON ---\n",
            "[\n",
            "  {\n",
            "    \"attribute\": \"color\",\n",
            "    \"value\": \"red\"\n",
            "  },\n",
            "  {\n",
            "    \"attribute\": \"material\",\n",
            "    \"value\": \"leather\"\n",
            "  },\n",
            "  {\n",
            "    \"attribute\": \"weight\",\n",
            "    \"value\": \"5 lbs\"\n",
            "  }\n",
            "]\n",
            "\n",
            "📝 Inference 2\n",
            "<s>You are an information extraction system. Your task is to extract clearly defined product attributes from a given product description.\n",
            "\n",
            "### Context:\n",
            "The goal is to identify and extract each distinct attribute of a product (such as color, material, weight, etc.) as a flat list of key-value pairs. Do not nest keys or group attributes. All information should be flattened with one key and one value per line in JSON format.\n",
            "\n",
            "### Instruction:\n",
            "Extract product attributes from the following description. Give the output as single key-value pairs in flat JSON format. Do NOT create nested or grouped structures.\n",
            "\n",
            "### Input:\n",
            "Extract product attributes from the description\n",
            "\n",
            "### Output:\n",
            "[\n",
            " {\n",
            "   \"attribute\": \"color\",\n",
            "   \"value\": \"red\"\n",
            " },\n",
            " {\n",
            "   \"attribute\": \"material\",\n",
            "   \"value\": \"leather\"\n",
            " },\n",
            " {\n",
            "   \"attribute\": \"weight\",\n",
            "   \"value\": \"5 lbs\"\n",
            " }\n",
            "]</s>\n",
            "\n",
            "--- Extracted JSON ---\n",
            "[\n",
            "  {\n",
            "    \"attribute\": \"color\",\n",
            "    \"value\": \"red\"\n",
            "  },\n",
            "  {\n",
            "    \"attribute\": \"material\",\n",
            "    \"value\": \"leather\"\n",
            "  },\n",
            "  {\n",
            "    \"attribute\": \"weight\",\n",
            "    \"value\": \"5 lbs\"\n",
            "  }\n",
            "]\n",
            "\n",
            "📝 Inference 3\n",
            "<s>You are an information extraction system. Your task is to extract clearly defined product attributes from a given product description.\n",
            "\n",
            "### Context:\n",
            "The goal is to identify and extract each distinct attribute of a product (such as color, material, weight, etc.) as a flat list of key-value pairs. Do not nest keys or group attributes. All information should be flattened with one key and one value per line in JSON format.\n",
            "\n",
            "### Instruction:\n",
            "Extract product attributes from the following description. Give the output as single key-value pairs in flat JSON format. Do NOT create nested or grouped structures.\n",
            "\n",
            "### Input:\n",
            "Extract product attributes from the description\n",
            "\n",
            "### Output:\n",
            "[\n",
            " {\n",
            "   \"attribute\": \"color\",\n",
            "   \"value\": \"red\"\n",
            " },\n",
            " {\n",
            "   \"attribute\": \"material\",\n",
            "   \"value\": \"leather\"\n",
            " },\n",
            " {\n",
            "   \"attribute\": \"weight\",\n",
            "   \"value\": \"5 lbs\"\n",
            " }\n",
            "]</s>\n",
            "\n",
            "--- Extracted JSON ---\n",
            "[\n",
            "  {\n",
            "    \"attribute\": \"color\",\n",
            "    \"value\": \"red\"\n",
            "  },\n",
            "  {\n",
            "    \"attribute\": \"material\",\n",
            "    \"value\": \"leather\"\n",
            "  },\n",
            "  {\n",
            "    \"attribute\": \"weight\",\n",
            "    \"value\": \"5 lbs\"\n",
            "  }\n",
            "]\n",
            "\n",
            "✅ All inferences complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vnn8Nc2Y27iB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "232ccecf29924c99ad700a3031fe9a0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2ae4bd0afabe4b928968dff5934559e9",
              "IPY_MODEL_80e93fb5162a4add9f0d78ff7294bf01",
              "IPY_MODEL_85dc7821d7a846698fce85f479891a7a"
            ],
            "layout": "IPY_MODEL_887f72dff3af454e92258e08a236308b"
          }
        },
        "2ae4bd0afabe4b928968dff5934559e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_468dfad3fe3a4662a9ffb00860ff80f4",
            "placeholder": "​",
            "style": "IPY_MODEL_fe90ade268ed42dbb399d306de193a63",
            "value": "Generating train split: "
          }
        },
        "80e93fb5162a4add9f0d78ff7294bf01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53eb9877d3514ff989f2c19a42f98bfb",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_84614b6e12a44e2e8b4f0f99c7294a40",
            "value": 1
          }
        },
        "85dc7821d7a846698fce85f479891a7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ff517cff33d4a81b5e81fb9e7a93844",
            "placeholder": "​",
            "style": "IPY_MODEL_114eeb3a28ec4faba7dd175ec7d85b65",
            "value": " 30/0 [00:00&lt;00:00, 549.94 examples/s]"
          }
        },
        "887f72dff3af454e92258e08a236308b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "468dfad3fe3a4662a9ffb00860ff80f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe90ade268ed42dbb399d306de193a63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53eb9877d3514ff989f2c19a42f98bfb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "84614b6e12a44e2e8b4f0f99c7294a40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3ff517cff33d4a81b5e81fb9e7a93844": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "114eeb3a28ec4faba7dd175ec7d85b65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "948379fd6f4041c4948b896c9a2c4f09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3d67d845496a4541b681ec2891e381f4",
              "IPY_MODEL_7ef06e09ecfe48ca9b3d6b5903b92aa4",
              "IPY_MODEL_e8e3cf02220b4a2fb4216d42b8a41639"
            ],
            "layout": "IPY_MODEL_1e55f112ba2d4a1d8f6416c4e117080a"
          }
        },
        "3d67d845496a4541b681ec2891e381f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd43c4568e8f4551bface94a7c393ea8",
            "placeholder": "​",
            "style": "IPY_MODEL_d8f00030e8094587b062673d0a9305d8",
            "value": "Map: 100%"
          }
        },
        "7ef06e09ecfe48ca9b3d6b5903b92aa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8db99cde17974e1396746c91513a0584",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c38ea39f94eb43fe9be3bac3470112db",
            "value": 30
          }
        },
        "e8e3cf02220b4a2fb4216d42b8a41639": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1de9fdd74a84cb98a50ed6374b9ca43",
            "placeholder": "​",
            "style": "IPY_MODEL_9a93699bd41c4eb492ea7a598ed255cf",
            "value": " 30/30 [00:00&lt;00:00, 302.75 examples/s]"
          }
        },
        "1e55f112ba2d4a1d8f6416c4e117080a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd43c4568e8f4551bface94a7c393ea8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8f00030e8094587b062673d0a9305d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8db99cde17974e1396746c91513a0584": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c38ea39f94eb43fe9be3bac3470112db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c1de9fdd74a84cb98a50ed6374b9ca43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a93699bd41c4eb492ea7a598ed255cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}